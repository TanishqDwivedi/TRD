{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each team may have 1~4 students. Top 10% teams worldwide as of 11:59pm on May 11, 2022 will get A in this class.\n",
    "Feature engineering: create new features from the raw data.\n",
    "\n",
    "Supervised learning models: Choose 2 supervised learning models from W3, W4, W5.\n",
    "\n",
    "Advanced models: choose 1 from XGBoost W12 or Neural Network W13 or Deep learning W14 to build your supervised learning models.\n",
    "\n",
    "Model evaluation: For each of the supervised learning models, evaluate the model using the techniques from W10\n",
    "\n",
    "If data size is too big and your model is too slow, feel free to choose small set for your project.\n",
    "\n",
    "Your jupyter submission should be in an article quality.\n",
    "\n",
    "Do NOT print huge data set in the notebook. Use head(). Use plotting to visualize your analysis and results. Use markdown to write your comments.\n",
    "\n",
    "If you are not in the top 10% teams worldwide, your score is based on completeness on every step as described above.\n",
    "\n",
    "Submit in HTML and ipynb format on canvas.<font/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size='5'>Group Members:<font/>\n",
    "    <br>\n",
    "<font size='5'>i)Aditya Maniar(am3326) <font/>\n",
    "    <br>\n",
    "    <font size='5'>                       ii)Tanishq Dwivedi(td43) <font/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import numpy as np \n",
    "import re\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import ngrams\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer ,CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential,Model\n",
    "from keras.layers import Flatten,Dense,LSTM,Dropout,BatchNormalization,Bidirectional,Embedding, Conv1D,MaxPooling1D, GlobalMaxPooling1D\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data=pd.read_csv('../input/nlpgettingstarted/train.csv')\n",
    "test_data=pd.read_csv('../input/nlpgettingstarted/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/kc/02ft3wl53qz2hk5l6mgj33r80000gn/T/ipykernel_1486/433269627.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'train_data' is not defined"
     ]
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/kc/02ft3wl53qz2hk5l6mgj33r80000gn/T/ipykernel_1486/1479869340.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'train_data' is not defined"
     ]
    }
   ],
   "source": [
    "train_data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-12T03:13:34.635287Z",
     "iopub.status.busy": "2022-05-12T03:13:34.634439Z",
     "iopub.status.idle": "2022-05-12T03:13:34.657728Z",
     "shell.execute_reply": "2022-05-12T03:13:34.656812Z",
     "shell.execute_reply.started": "2022-05-12T03:13:34.635224Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Number of Unique Keywords : ',len(train_data.keyword.unique()))\n",
    "train_data.keyword.value_counts(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-12T03:13:35.39984Z",
     "iopub.status.busy": "2022-05-12T03:13:35.399335Z",
     "iopub.status.idle": "2022-05-12T03:13:35.406249Z",
     "shell.execute_reply": "2022-05-12T03:13:35.405213Z",
     "shell.execute_reply.started": "2022-05-12T03:13:35.399794Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data.drop('id',inplace=True,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-12T03:13:37.227386Z",
     "iopub.status.busy": "2022-05-12T03:13:37.227096Z",
     "iopub.status.idle": "2022-05-12T03:13:37.234961Z",
     "shell.execute_reply": "2022-05-12T03:13:37.234135Z",
     "shell.execute_reply.started": "2022-05-12T03:13:37.227355Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data.target.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"6\">Preprocessing of text data</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-12T03:13:39.694267Z",
     "iopub.status.busy": "2022-05-12T03:13:39.693672Z",
     "iopub.status.idle": "2022-05-12T03:13:39.699471Z",
     "shell.execute_reply": "2022-05-12T03:13:39.698372Z",
     "shell.execute_reply.started": "2022-05-12T03:13:39.694231Z"
    }
   },
   "outputs": [],
   "source": [
    "#convert tweets to lower_case\n",
    "def lower_case(text):\n",
    "    text = [' '.join(tx.lower() for tx in word.split()) for word in text]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-12T03:13:40.261847Z",
     "iopub.status.busy": "2022-05-12T03:13:40.26152Z",
     "iopub.status.idle": "2022-05-12T03:13:40.297646Z",
     "shell.execute_reply": "2022-05-12T03:13:40.296684Z",
     "shell.execute_reply.started": "2022-05-12T03:13:40.261811Z"
    }
   },
   "outputs": [],
   "source": [
    "# Make A List Of All Words \n",
    "words=[]\n",
    "for i in train_data.text:\n",
    "    words.extend(i.split())\n",
    "print('Unique words:',len(set(words)))\n",
    "print('total words:',len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-12T03:13:40.697964Z",
     "iopub.status.busy": "2022-05-12T03:13:40.697084Z",
     "iopub.status.idle": "2022-05-12T03:13:40.705245Z",
     "shell.execute_reply": "2022-05-12T03:13:40.704202Z",
     "shell.execute_reply.started": "2022-05-12T03:13:40.697902Z"
    }
   },
   "outputs": [],
   "source": [
    "# remove #retweet from tweets \n",
    "def retweets(text):\n",
    "    tweets = [' '.join([re.sub('^@(\\w)+',' ',tw) for tw in tweet.split()]) for tweet in text]  \n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-12T03:13:41.138911Z",
     "iopub.status.busy": "2022-05-12T03:13:41.137969Z",
     "iopub.status.idle": "2022-05-12T03:13:41.145732Z",
     "shell.execute_reply": "2022-05-12T03:13:41.144861Z",
     "shell.execute_reply.started": "2022-05-12T03:13:41.138852Z"
    }
   },
   "outputs": [],
   "source": [
    "#remove url's from tweets\n",
    "def drop_url(text):\n",
    "    url = r\"(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?«»“”‘’]))\"\n",
    "    tweets = [' '.join([re.sub(url,' ',tw) for tw in tweet.split()]) for tweet in text]  \n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-12T03:13:41.567971Z",
     "iopub.status.busy": "2022-05-12T03:13:41.566839Z",
     "iopub.status.idle": "2022-05-12T03:13:41.574154Z",
     "shell.execute_reply": "2022-05-12T03:13:41.572957Z",
     "shell.execute_reply.started": "2022-05-12T03:13:41.567909Z"
    }
   },
   "outputs": [],
   "source": [
    "#lemmatizing \n",
    "def lemmatize(tweets):\n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    tweets = [' '.join(lemmatizer.lemmatize(word) for word in word_tokenize(tweet)) for tweet in tweets]\n",
    "    return tweets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-12T03:13:41.977545Z",
     "iopub.status.busy": "2022-05-12T03:13:41.976758Z",
     "iopub.status.idle": "2022-05-12T03:13:41.982709Z",
     "shell.execute_reply": "2022-05-12T03:13:41.981727Z",
     "shell.execute_reply.started": "2022-05-12T03:13:41.977506Z"
    }
   },
   "outputs": [],
   "source": [
    "#removing stop words\n",
    "def stop_words(tweets):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tweets = [' '.join(word for word in word_tokenize(tweet) if word not in stop_words) for tweet in tweets]\n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-12T03:13:42.40215Z",
     "iopub.status.busy": "2022-05-12T03:13:42.401628Z",
     "iopub.status.idle": "2022-05-12T03:13:42.407866Z",
     "shell.execute_reply": "2022-05-12T03:13:42.40607Z",
     "shell.execute_reply.started": "2022-05-12T03:13:42.4021Z"
    }
   },
   "outputs": [],
   "source": [
    "#final cleaning of tweets\n",
    "def final_cleaning(tweets):\n",
    "    tweets = [\" \".join(word_tokenize(tweet)) for tweet in tweets]\n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-12T03:13:42.824256Z",
     "iopub.status.busy": "2022-05-12T03:13:42.823603Z",
     "iopub.status.idle": "2022-05-12T03:13:42.831236Z",
     "shell.execute_reply": "2022-05-12T03:13:42.829929Z",
     "shell.execute_reply.started": "2022-05-12T03:13:42.824218Z"
    }
   },
   "outputs": [],
   "source": [
    "def vectorizing(number,corpus):\n",
    "    count_vec = CountVectorizer(ngram_range=(number, number)).fit(corpus)\n",
    "    bag_of_words = count_vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in count_vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-12T03:13:43.204184Z",
     "iopub.status.busy": "2022-05-12T03:13:43.203624Z",
     "iopub.status.idle": "2022-05-12T03:13:43.210106Z",
     "shell.execute_reply": "2022-05-12T03:13:43.20863Z",
     "shell.execute_reply.started": "2022-05-12T03:13:43.204146Z"
    }
   },
   "outputs": [],
   "source": [
    "# get final data according to target variable ie. 0 and 1 \n",
    "def final_data(target):\n",
    "    clean_data = lower_case(train_data[train_data[\"target\"] == target][\"text\"])\n",
    "    clean_data = retweets(clean_data)\n",
    "    clean_data = drop_url(clean_data)\n",
    "    clean_data = lemmatize(clean_data)\n",
    "    clean_data = stop_words(clean_data)\n",
    "    clean_data = final_cleaning(clean_data)\n",
    "    return clean_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size='6'>Using Count Vectorizer to find continuous sequences( in this case '1') of words or symbols or tokens in the clean text and then printing the frequency of each word.<font/>\n",
    "    <br>\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-12T03:13:44.48965Z",
     "iopub.status.busy": "2022-05-12T03:13:44.48937Z",
     "iopub.status.idle": "2022-05-12T03:13:52.102983Z",
     "shell.execute_reply": "2022-05-12T03:13:52.10218Z",
     "shell.execute_reply.started": "2022-05-12T03:13:44.48962Z"
    }
   },
   "outputs": [],
   "source": [
    "#for negative reactions\n",
    "vectorizing(1,final_data(0))[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-12T03:13:52.104923Z",
     "iopub.status.busy": "2022-05-12T03:13:52.10453Z",
     "iopub.status.idle": "2022-05-12T03:13:55.247478Z",
     "shell.execute_reply": "2022-05-12T03:13:55.246675Z",
     "shell.execute_reply.started": "2022-05-12T03:13:52.104884Z"
    }
   },
   "outputs": [],
   "source": [
    "#for positive reactions\n",
    "vectorizing(1,final_data(1))[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"6\">Next we will create a pipeline with all the above functions for smooth preprocessing of the train and test data.</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-12T03:16:45.659466Z",
     "iopub.status.busy": "2022-05-12T03:16:45.658735Z",
     "iopub.status.idle": "2022-05-12T03:16:45.665865Z",
     "shell.execute_reply": "2022-05-12T03:16:45.664931Z",
     "shell.execute_reply.started": "2022-05-12T03:16:45.659424Z"
    }
   },
   "outputs": [],
   "source": [
    "pipe1 = FunctionTransformer(lower_case)\n",
    "pipe2 = FunctionTransformer(retweets)\n",
    "pipe3 = FunctionTransformer(drop_url)\n",
    "pipe4 = FunctionTransformer(lemmatize)\n",
    "pipe5 = FunctionTransformer(final_cleaning)\n",
    "pipe6 = FunctionTransformer(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-12T03:24:05.782912Z",
     "iopub.status.busy": "2022-05-12T03:24:05.782605Z",
     "iopub.status.idle": "2022-05-12T03:24:05.792121Z",
     "shell.execute_reply": "2022-05-12T03:24:05.790297Z",
     "shell.execute_reply.started": "2022-05-12T03:24:05.78288Z"
    }
   },
   "outputs": [],
   "source": [
    "#This function will feed the pipeline to the different supervised models for prediction and classification report.  \n",
    "def preproc_pipeline(pipe):\n",
    "    pipeline = Pipeline(pipe)\n",
    "    \n",
    "    x =train_data.text.copy()\n",
    "    y =train_data.target.copy()\n",
    "    X_train ,X_test ,y_train,y_test =train_test_split(x,y,test_size=0.2,random_state=42)\n",
    "    \n",
    "    pipeline.fit(X_train,y_train)\n",
    "    target_preds =pipeline.predict(X_test)\n",
    "    print('Confusion Matrix: ')\n",
    "    print(confusion_matrix(y_test, target_preds))\n",
    "    print(classification_report(y_test,target_preds))\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"6\">Model 1: Using Support Vector Classification</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-12T03:24:07.277363Z",
     "iopub.status.busy": "2022-05-12T03:24:07.276691Z",
     "iopub.status.idle": "2022-05-12T03:24:20.729086Z",
     "shell.execute_reply": "2022-05-12T03:24:20.727831Z",
     "shell.execute_reply.started": "2022-05-12T03:24:07.277323Z"
    }
   },
   "outputs": [],
   "source": [
    "SVC_model = SVC()\n",
    "pipeline = [('lower',pipe1),('retweet',pipe2),('urls',pipe3),('lematize',pipe4),('spacs',pipe5),('tf_idf',TfidfVectorizer()),('model',SVC_model)]\n",
    "pipeline_svc = preproc_pipeline(pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size='6' color='red'>Accuracy of SVC model is 82%.  <font/>\n",
    "    <br>\n",
    "    <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size='6'>Model 2: Using Logistic Regression  <font/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-12T03:24:32.158833Z",
     "iopub.status.busy": "2022-05-12T03:24:32.158474Z",
     "iopub.status.idle": "2022-05-12T03:24:38.225278Z",
     "shell.execute_reply": "2022-05-12T03:24:38.224172Z",
     "shell.execute_reply.started": "2022-05-12T03:24:32.158776Z"
    }
   },
   "outputs": [],
   "source": [
    "logistic_reg=LogisticRegression()\n",
    "pipeline = [('lower',pipe1),('retweet',pipe2),('urls',pipe3),('lematize',pipe4),('spacs',pipe5),('tf_idf',TfidfVectorizer()),('model',logistic_reg)]\n",
    "pipeline_logistic_reg = preproc_pipeline(pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size='6' color='red'>Accuracy of Logistic Regression model is 81%. <font/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<font size='6'>Using a CNN model<font/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-12T03:24:44.625408Z",
     "iopub.status.busy": "2022-05-12T03:24:44.624757Z",
     "iopub.status.idle": "2022-05-12T03:24:51.321087Z",
     "shell.execute_reply": "2022-05-12T03:24:51.320167Z",
     "shell.execute_reply.started": "2022-05-12T03:24:44.625367Z"
    }
   },
   "outputs": [],
   "source": [
    "x = train_data.text.copy()\n",
    "x = lower_case(x)\n",
    "x = retweets(x)\n",
    "x = drop_url(x)\n",
    "x = lemmatize(x)\n",
    "x = final_cleaning(x) \n",
    "y =train_data.target.copy()\n",
    "\n",
    "tokenize = Tokenizer()\n",
    "tokenize.fit_on_texts(x)\n",
    "x = tokenize.texts_to_sequences(x)\n",
    "print(len(tokenize.word_index)+1)\n",
    "vocab_length =len(tokenize.index_word)+1\n",
    "x = pad_sequences(x,maxlen=np.max(vocab_length))\n",
    "X_train ,X_test ,y_train,y_test =train_test_split(x,y,test_size=0.2,random_state=42)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-12T03:25:06.7317Z",
     "iopub.status.busy": "2022-05-12T03:25:06.731407Z",
     "iopub.status.idle": "2022-05-12T03:25:10.389814Z",
     "shell.execute_reply": "2022-05-12T03:25:10.388993Z",
     "shell.execute_reply.started": "2022-05-12T03:25:06.73167Z"
    }
   },
   "outputs": [],
   "source": [
    "model =Sequential()\n",
    "\n",
    "model.add(Embedding(input_length=vocab_length,output_dim=32,input_dim=vocab_length))\n",
    "\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(GlobalMaxPooling1D())\n",
    "\n",
    "model.add(Dropout(0.7))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "\n",
    "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=0.001,\n",
    "    decay_steps=1000,\n",
    "    decay_rate=0.9)\n",
    "es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='auto',patience=3,restore_best_weights=True)\n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "              ,loss='binary_crossentropy',metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-12T03:25:26.381191Z",
     "iopub.status.busy": "2022-05-12T03:25:26.380898Z",
     "iopub.status.idle": "2022-05-12T03:26:30.672729Z",
     "shell.execute_reply": "2022-05-12T03:26:30.67192Z",
     "shell.execute_reply.started": "2022-05-12T03:25:26.38116Z"
    }
   },
   "outputs": [],
   "source": [
    "history = model.fit(X_train,y_train,\n",
    "                    epochs=15,verbose=1,validation_data = (X_test,y_test),callbacks=es)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-12T03:26:39.466427Z",
     "iopub.status.busy": "2022-05-12T03:26:39.466119Z",
     "iopub.status.idle": "2022-05-12T03:26:40.060912Z",
     "shell.execute_reply": "2022-05-12T03:26:40.059803Z",
     "shell.execute_reply.started": "2022-05-12T03:26:39.466384Z"
    }
   },
   "outputs": [],
   "source": [
    "model.summary()\n",
    "loss, acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test Accuracy: %f' % (acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-12T03:30:09.055533Z",
     "iopub.status.busy": "2022-05-12T03:30:09.055132Z",
     "iopub.status.idle": "2022-05-12T03:30:09.836608Z",
     "shell.execute_reply": "2022-05-12T03:30:09.835246Z",
     "shell.execute_reply.started": "2022-05-12T03:30:09.055495Z"
    }
   },
   "outputs": [],
   "source": [
    "cnn_preds=model.predict(X_test)\n",
    "for i in range(0,len(cnn_preds)):\n",
    "    if(cnn_preds[i]>0.5):\n",
    "        cnn_preds[i]=1\n",
    "    else:\n",
    "        cnn_preds[i]=0\n",
    "print('Confusion Matrix: ')\n",
    "print(confusion_matrix(y_test, cnn_preds))\n",
    "print(classification_report(y_test,cnn_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size='6'>Prediction using CNN model<font/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-12T03:30:30.136729Z",
     "iopub.status.busy": "2022-05-12T03:30:30.136433Z",
     "iopub.status.idle": "2022-05-12T03:30:32.575562Z",
     "shell.execute_reply": "2022-05-12T03:30:32.574753Z",
     "shell.execute_reply.started": "2022-05-12T03:30:30.136696Z"
    }
   },
   "outputs": [],
   "source": [
    "a = test_data.text.copy()\n",
    "a = lower_case(a)\n",
    "a = retweets(a)\n",
    "a = drop_url(a)\n",
    "a = lemmatize(a)\n",
    "a = final_cleaning(a) \n",
    "\n",
    "tokenize = Tokenizer()\n",
    "tokenize.fit_on_texts(a)\n",
    "a = tokenize.texts_to_sequences(a)\n",
    "print(len(tokenize.word_index)+1)\n",
    "vocab_length =len(tokenize.index_word)+1\n",
    "a = pad_sequences(a,maxlen=np.max(vocab_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-12T03:30:35.296063Z",
     "iopub.status.busy": "2022-05-12T03:30:35.295294Z",
     "iopub.status.idle": "2022-05-12T03:30:36.10788Z",
     "shell.execute_reply": "2022-05-12T03:30:36.107043Z",
     "shell.execute_reply.started": "2022-05-12T03:30:35.296019Z"
    }
   },
   "outputs": [],
   "source": [
    "final_preds=model.predict(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-12T03:30:41.336585Z",
     "iopub.status.busy": "2022-05-12T03:30:41.33588Z",
     "iopub.status.idle": "2022-05-12T03:30:41.353234Z",
     "shell.execute_reply": "2022-05-12T03:30:41.351999Z",
     "shell.execute_reply.started": "2022-05-12T03:30:41.33654Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(0,len(final_preds)):\n",
    "    if(final_preds[i]>0.5):\n",
    "        final_preds[i]=1\n",
    "    else:\n",
    "        final_preds[i]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-12T03:30:43.679649Z",
     "iopub.status.busy": "2022-05-12T03:30:43.679378Z",
     "iopub.status.idle": "2022-05-12T03:30:43.684468Z",
     "shell.execute_reply": "2022-05-12T03:30:43.68365Z",
     "shell.execute_reply.started": "2022-05-12T03:30:43.679619Z"
    }
   },
   "outputs": [],
   "source": [
    "final_preds=np.array(final_preds,int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-11T21:22:43.150748Z",
     "iopub.status.busy": "2022-05-11T21:22:43.150317Z",
     "iopub.status.idle": "2022-05-11T21:22:43.179158Z",
     "shell.execute_reply": "2022-05-11T21:22:43.178477Z",
     "shell.execute_reply.started": "2022-05-11T21:22:43.150709Z"
    }
   },
   "outputs": [],
   "source": [
    "sample1 = pd.read_csv('../input/nlpgettingstarted/sample_submission.csv')\n",
    "sample1['target']=final_preds\n",
    "sample1.to_csv('submission1.csv',index = False)\n",
    "sample1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### <font size='6'>Prediction using SVC model<font/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-12T03:31:18.02388Z",
     "iopub.status.busy": "2022-05-12T03:31:18.023615Z",
     "iopub.status.idle": "2022-05-12T03:31:23.251751Z",
     "shell.execute_reply": "2022-05-12T03:31:23.250798Z",
     "shell.execute_reply.started": "2022-05-12T03:31:18.023852Z"
    }
   },
   "outputs": [],
   "source": [
    "test_pred=test_data.text\n",
    "test_prediction = pipeline_svc.predict(test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-11T21:24:39.537361Z",
     "iopub.status.busy": "2022-05-11T21:24:39.537098Z",
     "iopub.status.idle": "2022-05-11T21:24:39.56047Z",
     "shell.execute_reply": "2022-05-11T21:24:39.559772Z",
     "shell.execute_reply.started": "2022-05-11T21:24:39.537331Z"
    }
   },
   "outputs": [],
   "source": [
    "sample2 = pd.read_csv('../input/nlpgettingstarted/sample_submission.csv')\n",
    "sample2['target']=test_prediction\n",
    "sample2.to_csv('submission2.csv',index = False)\n",
    "sample2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size='6'>Using BertTokenizer and Bert Model for Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-11T21:27:50.597701Z",
     "iopub.status.busy": "2022-05-11T21:27:50.597424Z",
     "iopub.status.idle": "2022-05-11T21:27:53.204399Z",
     "shell.execute_reply": "2022-05-11T21:27:53.203658Z",
     "shell.execute_reply.started": "2022-05-11T21:27:50.597671Z"
    }
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import BertTokenizer\n",
    "from transformers import TFBertModel\n",
    "import random,os\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 2022\n",
    "def seed_everything(seed=SEED):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-11T21:25:44.040926Z",
     "iopub.status.busy": "2022-05-11T21:25:44.040032Z",
     "iopub.status.idle": "2022-05-11T21:25:45.783881Z",
     "shell.execute_reply": "2022-05-11T21:25:45.783115Z",
     "shell.execute_reply.started": "2022-05-11T21:25:44.04087Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-large-uncased\", do_lower_case=True)\n",
    "def bert_encode(data,maximum_length) :\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    \n",
    "    for i in range(len(data.text)):\n",
    "        encoded = tokenizer.encode_plus(\n",
    "\n",
    "        data.text[i],\n",
    "        add_special_tokens=True,\n",
    "        max_length=maximum_length,\n",
    "        pad_to_max_length=True,\n",
    "\n",
    "        return_attention_mask=True,\n",
    "\n",
    "        )\n",
    "\n",
    "        input_ids.append(encoded['input_ids'])\n",
    "        attention_masks.append(encoded['attention_mask'])\n",
    "\n",
    "    return np.array(input_ids),np.array(attention_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-11T21:25:55.660614Z",
     "iopub.status.busy": "2022-05-11T21:25:55.659937Z",
     "iopub.status.idle": "2022-05-11T21:26:04.472362Z",
     "shell.execute_reply": "2022-05-11T21:26:04.471583Z",
     "shell.execute_reply.started": "2022-05-11T21:25:55.660576Z"
    }
   },
   "outputs": [],
   "source": [
    "train_input_ids,train_attention_masks = bert_encode(train_data,60)\n",
    "test_input_ids,test_attention_masks = bert_encode(test_data,60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-11T21:26:08.289813Z",
     "iopub.status.busy": "2022-05-11T21:26:08.289141Z",
     "iopub.status.idle": "2022-05-11T21:26:08.296433Z",
     "shell.execute_reply": "2022-05-11T21:26:08.295607Z",
     "shell.execute_reply.started": "2022-05-11T21:26:08.289774Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_model(bert_model):\n",
    "    input_ids = tf.keras.Input(shape=(60,),dtype='int32')\n",
    "    attention_masks = tf.keras.Input(shape=(60,),dtype='int32')\n",
    "\n",
    "    output = bert_model([input_ids,attention_masks])\n",
    "    output = output[1]\n",
    "    output = tf.keras.layers.Dense(32,activation='relu')(output)\n",
    "    output = tf.keras.layers.Dropout(0.2)(output)\n",
    "\n",
    "    output = tf.keras.layers.Dense(1,activation='sigmoid')(output)\n",
    "    model = tf.keras.models.Model(inputs = [input_ids,attention_masks],outputs = output)\n",
    "    model.compile(tensorflow.keras.optimizers.Adam(lr=6e-6), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = TFBertModel.from_pretrained(\"bert-large-uncased\")\n",
    "\n",
    "model = create_model(bert_model)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-11T20:44:06.15245Z",
     "iopub.status.busy": "2022-05-11T20:44:06.152116Z",
     "iopub.status.idle": "2022-05-11T20:44:06.226396Z",
     "shell.execute_reply": "2022-05-11T20:44:06.225457Z",
     "shell.execute_reply.started": "2022-05-11T20:44:06.152363Z"
    }
   },
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    [train_input_ids,train_attention_masks],\n",
    "    train_data['target'],\n",
    "    validation_split=0.2,\n",
    "    epochs=2,\n",
    "    batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = model.predict([test_input_ids,test_attention_masks])\n",
    "\n",
    "sub = pd.read_csv('../input/nlpgettingstarted/sample_submission.csv')\n",
    "sub['target'] = np.round(pred_test).astype(int)\n",
    "sub.to_csv('final_submission',index=False)\n",
    "sub.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size='6'> We used SVC, Logistic Regression, CN, and a pretrained Model (Bert Model) for prediction. Since Bert Model is giving maximum accuracy while prediciting, we will use the predictions of the Bert Model for the Kaggle Competietion.<font/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
